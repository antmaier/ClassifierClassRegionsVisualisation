{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/antoine/anaconda3/envs/Emergence/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from Modules import Autoencoder\n",
    "from Modules import LinearAutoencoder\n",
    "from LossFunctions import tan_square, log_product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device = 'cpu'\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are only interested to train autoencoders, so we flatten the 2dim of the images and\n",
    "# squeeze the single channel dimension\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Lambda(lambda x: torch.squeeze(x)),\n",
    "    transforms.Lambda(lambda x: torch.flatten(x, start_dim=-2))])\n",
    "\n",
    "train_dataset = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_dataset = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# Train and test data tensors loaded on the GPU\n",
    "train_data = train_dataset.data.clone().detach().type(torch.float32).flatten(start_dim=-2).to(device).share_memory_()\n",
    "test_data = test_dataset.data.clone().detach().type(torch.float32).flatten(start_dim=-2).to(device).share_memory_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 3\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(train_dataset), size=(1,)).item()\n",
    "    img, label = train_dataset[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(label)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.reshape((28, 28)), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: torch.Size([8, 784])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "for X, _ in train_dataloader:\n",
    "    print(f\"Shape of X: {X.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identity Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IdentityTransformer(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        # https://stackoverflow.com/questions/60908827/why-pytorch-nn-module-cuda-not-moving-module-tensor-but-only-parameters-and-bu\n",
    "        self.positional_encoding = nn.Parameter(torch.rand((28), requires_grad=True)) # TODO see if requires_grad=True is needed here\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=28, \n",
    "            nhead=1, \n",
    "            num_encoder_layers=3, \n",
    "            num_decoder_layers=3, \n",
    "            batch_first=True) # https://discuss.pytorch.org/t/why-is-sequence-batch-features-the-default-instead-of-bxsxf/8244\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"Takes a batch of 28*28 pictures (N,28,28) # TODO verifier que c'est pas (N,1,28,28) que donne le dataloader\n",
    "        We want to have (N,S,E)=(N,28,28), i.e. each row is a 'token' and the whole picture is a sentence.\"\"\"\n",
    "        input = torch.add(input, self.positional_encoding) # picture += positional encoding\n",
    "        # input = self.flatten(input) # (N,28,28) -> (N,784)\n",
    "        # input = torch.unsqueeze(input, 2) # (N,784) -> (N,784,1)\n",
    "        input = torch.squeeze(input) # TODO lié au TODO d'en haut (N,1,28,28)->(N,28,28)\n",
    "        output = self.transformer(input, input)\n",
    "        # output = torch.reshape(output, input.shape) # (N,1,784) -> (N,28,28)\n",
    "        return output\n",
    "\n",
    "# identity_transformer = IdentityTransformer().to(device)\n",
    "\n",
    "# loss_function = nn.MSELoss()\n",
    "# optimizer = torch.optim.Adam(identity_transformer.parameters(), lr=1e-4) # TODO train learning rate during training phase r adjust it \n",
    "# https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoencoder = Autoencoder(dim_sequence=28*28, dim_bottleneck=2, dim_positional_encoding=2, num_heads=2, num_layers=2).to(device)\n",
    "linear_autoencoder = LinearAutoencoder(dim_input=28*28, dim_bottleneck=2, step= 10, activation_function='ReLU').to(device)\n",
    "loss_function = nn.MSELoss()\n",
    "# loss_function = log_product\n",
    "optimizer = torch.optim.Adam(linear_autoencoder.parameters(), lr=1e-3, weight_decay=0.01)\n",
    "# TODO tellement de trucs à fine-tuner ici..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_autoencoder(dataloader: DataLoader, model, loss_function, optimizer):\n",
    "    \"\"\"Right now, when iterated over, the dataloader returns a data point AND a label. \n",
    "    TODO faire en sorte que le dataloader soit composé que des images, sans les labels.\"\"\"\n",
    "\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, _) in enumerate(dataloader):\n",
    "        X = X.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_function(pred, X)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def test_autoencoder(dataloader, model, loss_function):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X, _ in dataloader:\n",
    "            X = X.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_function(pred, X).item()\n",
    "    test_loss /= num_batches\n",
    "    print(f\"Test Error: Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "def train_linear_autoencoder(data, model, loss_function, optimizer):\n",
    "    model.train()\n",
    "    pred = model(data)\n",
    "    loss = loss_function(pred, data)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(\"Train loss:\", loss)\n",
    "\n",
    "def test_linear_autoencoder(data, model, loss_function):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred = model(data)\n",
    "        loss = loss_function(pred, data)\n",
    "\n",
    "    print(\"Test loss:\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: unknown error\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/antoine/workspace/ClassifierClassRegionsVisualisation/Model.ipynb Cell 16\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/antoine/workspace/ClassifierClassRegionsVisualisation/Model.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/antoine/workspace/ClassifierClassRegionsVisualisation/Model.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mEpoch:\u001b[39m\u001b[39m\"\u001b[39m, t)\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/antoine/workspace/ClassifierClassRegionsVisualisation/Model.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     train_linear_autoencoder(train_data, linear_autoencoder, loss_function, optimizer)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/antoine/workspace/ClassifierClassRegionsVisualisation/Model.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     test_linear_autoencoder(test_data, linear_autoencoder, loss_function)\n",
      "\u001b[1;32m/home/antoine/workspace/ClassifierClassRegionsVisualisation/Model.ipynb Cell 16\u001b[0m in \u001b[0;36mtrain_linear_autoencoder\u001b[0;34m(data, model, loss_function, optimizer)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/antoine/workspace/ClassifierClassRegionsVisualisation/Model.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain_linear_autoencoder\u001b[39m(data, model, loss_function, optimizer):\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/antoine/workspace/ClassifierClassRegionsVisualisation/Model.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=36'>37</a>\u001b[0m     model\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/antoine/workspace/ClassifierClassRegionsVisualisation/Model.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m     pred \u001b[39m=\u001b[39m model(data)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/antoine/workspace/ClassifierClassRegionsVisualisation/Model.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=38'>39</a>\u001b[0m     loss \u001b[39m=\u001b[39m loss_function(pred, data)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/antoine/workspace/ClassifierClassRegionsVisualisation/Model.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=40'>41</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/anaconda3/envs/Emergence/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/workspace/ClassifierClassRegionsVisualisation/Modules.py:121\u001b[0m, in \u001b[0;36mLinearAutoencoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m    120\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder:\n\u001b[0;32m--> 121\u001b[0m         x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactivation_function(module(x))\n\u001b[1;32m    122\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbottleneck(x)\n\u001b[1;32m    123\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder:\n",
      "File \u001b[0;32m~/anaconda3/envs/Emergence/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/Emergence/lib/python3.10/site-packages/torch/nn/modules/activation.py:102\u001b[0m, in \u001b[0;36mReLU.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 102\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mrelu(\u001b[39minput\u001b[39;49m, inplace\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minplace)\n",
      "File \u001b[0;32m~/anaconda3/envs/Emergence/lib/python3.10/site-packages/torch/nn/functional.py:1457\u001b[0m, in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1455\u001b[0m     result \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrelu_(\u001b[39minput\u001b[39m)\n\u001b[1;32m   1456\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1457\u001b[0m     result \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mrelu(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m   1458\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: unknown error\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "# Train linear autoencoder\n",
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(\"Epoch:\", t)\n",
    "    train_linear_autoencoder(train_data, linear_autoencoder, loss_function, optimizer)\n",
    "    test_linear_autoencoder(test_data, linear_autoencoder, loss_function)\n",
    "\n",
    "\n",
    "# TODO faire que le dataset est sur la shared memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_autoencoder(train_dataloader, autoencoder, loss_function, optimizer)\n",
    "    test_autoencoder(test_dataloader, autoencoder, loss_function)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5\n",
    "fig, axs = plt.subplots(n, 2, figsize=(2,5))\n",
    "for i in range(n):\n",
    "    # Input\n",
    "    random_index = np.random.randint(len(test_data))\n",
    "    img, label = test_data[random_index]\n",
    "    ax = axs[i, 0]\n",
    "    ax.imshow(img.squeeze(), cmap='gray')\n",
    "    ax.axis(\"off\")\n",
    "    if i == 0: ax.set_title('Input')\n",
    "\n",
    "    # Output\n",
    "    input_image = img.to(device).squeeze().flatten(start_dim=-2)\n",
    "    output_image = autoencoder(input_image)\n",
    "    output_image = output_image.reshape(img.shape)\n",
    "\n",
    "    ax = axs[i, 1]\n",
    "    ax.imshow(output_image.cpu().detach().numpy().squeeze(), cmap=\"gray\")\n",
    "    ax.axis(\"off\")\n",
    "    if i == 0: ax.set_title('Output')\n",
    "plt.subplots_adjust(wspace=0.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO faire un environnement \"try\" ou quand on quitte l'environnement on enregistre le modèle dans l'état actuel, la training curve, etc\n",
    "# vérifier que si Windows décide d'update il quitte python de façon clean et lui laisse le temps d'enregistrer et tout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "del(X)\n",
    "del(y)\n",
    "del(autoencoder)\n",
    "gc.collect()\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('Emergence')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8b1d9945a33258b0ffd7aeaf6809be58234033e79986f5ef0464e80e152d3eb4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
